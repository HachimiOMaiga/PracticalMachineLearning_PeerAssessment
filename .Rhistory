adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
dim(training)
adData1 = data.frame(diagnosis,predictors)
trainIndex1 = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training1 = adData[trainIndex1,]
testing1 = adData[-trainIndex,]
dim(training1)
adData2 = data.frame(predictors)
trainIndex2 = createDataPartition(diagnosis,p=0.5,list=FALSE)
training2 = adData[trainIndex2,]
testing2 = adData2[-trainIndex2,]
training2 = adData2[trainIndex2,]
dim(training2)
training1 = adData1[trainIndex1,]
testing1 = adData1[-trainIndex,]
dim(training1)
data(concrete)
View(concrete)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training)
dim(testing)
hist(concrete$Superplasticizer)
hist(log(concrete$Superplasticizer))
hist(concrete$Superplasticizer)
hist(log(concrete$Superplasticizer))
hist(concrete$Superplasticizer)
mean(concrete$Superplasticizer)
sd(concrete$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
dim(training)
dim(testing)
View(training)
View(testing)
training[, grep("IL", colnames(training))]
dim(training[, grep("IL", colnames(training))])
dim(training)
131 - 13
M <- abs(cor(training, -118))
M <- abs(cor(training[, -118]))
M <- abs(cor(training[,-118]))
M <- abs(x = cor(training[,-118]))
library(caret)
M <- abs(x = cor(training[,-118]))
typeof(training[, -118])
M <- abs(x = cor(training[,-118], use = "pairwise.complete.obs"))
M <- abs(x = cor(as.numeric(training[, -118])))
typeof(training)
training = adData[inTrain,]
dim(training)
dim(training[, grep("IL", colnames(training))])
training[, grep("IL", colnames(training))]
M <- abs(x = cor(training[, -118]))
M <- abs(cor(training[, -118]))
M <- abs(cor(training[,-118]))
training[,grepl("^ML", names(training))]
training[,grep("^ML", names(training))]
training[,grep("^IL", names(training))]
dim(training[, grep("^IL", names(training))])
dim(training)
131-12
M <- abs(cor(training[,-119]))
preProc <- preProcess(training[, -119]+1, method = "pca", pcaComp = 12)
preProc <- preProcess(log10(training[, -119]+1), method = "pca", pcaComp = 12)
grep("^IL", colnames(training), value = TRUE)
varStartingWithIL <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, -varStartingWithIL], method = "pca", thresh = 0.8)
preProc <- preProcess(training[, varStartingWithIL], method = "pca", thresh = 0.8)
preProc
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
install.packages("e1071")
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confustion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
A1 <- C1$overall[1]
## do similar steps with the caret package
modelFit <- train(training$diagnosis ~ ., method = "glm", preProcess = "pca",
data = training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
print(C2)
A2 <- C2$overall[1]
A2
data(mtcars)
View(mtcars)
data(mtcars)
y <- mtcars$mpg
x <- mtcars$cyl
t <- mtcars$wt
carLM <- lm(y ~ x + t)
summary(carLM)$coefficients
summary(carLM)
carLM <- lm(y ~ factor(x) + t)
summary(carLM)$coefficients
coefficients(carLM)
adjustedModel <- lm(y ~ factor(x)+t)
unadjustedModel <- lm(y ~ factor(x))
summary(adjustedModel)$coefficients
summary(unadjustedModel)$coefficients
mean(mtcars[,mtcars$cyl == 8])
tapply(X = mtcars$mpg, INDEX = mtcars$cyl, FUN = mean)
26.66364 - 19.74286
26.66364 - 15.10000
summary(adjustedModel)$coefficients
interactionModel <- lm(y ~ x + t + x:t)
summary(interactionModel)$coefficients
interactionModel <- lm(y ~ factor(x) + t + factor(x):t)
summary(adjustedModel)$coefficients
summary(interactionModel)$coefficients
Statistical Inference Course Project - Part 1
=============================================
---------------------------------------------
Title: "Exponential Distribution Simulation"
Author: "Hachimi O. Maiga"
Date: "Wednesday, June 15, 2015"
---------------------------------------------
---------------------------------------------
The aim of this report is to  use simulation to explore inference in order to investigate the exponential distribution and compare it with the Central Limit Theorem (CLT). The distribution of sample means will be explored by drawing samples from an exponential distribution. The centrality of the distribution will be looked at and compared to the theoretical mean of the distribution. Then the variability of the distribution of sample means is assessed and compared to the theoretical variance of the distribution. Finally, the distribution closeness to the Gaussian distribution is explored.
## Properties of the distribution of the mean of 40 exponentials
For the purpose of this report, a few parameters are set and fixed throughout the report. The rate parameter
is set to lamda = 0.2, the number of observations is 40 and the number of simulations is 1000. In order to make the simulation reproducible, so that the reader can generate the same sample for herself, the seed is set to 1 before the sample is generated.
--------------------------------------
**Initialising and setting constants**
--------------------------------------
```{r}
numberOfObservations <- 40
numberOfSimulations <- 1000
lamda <- 0.2
```
1. Sample mean vs theoretical mean of the distribution
------------------------------------------------------
```{r}
theoreticalMean <- 1/lamda
```
The theoretical mean of the distribution is **`r theoreticalMean`**
---------------------------------------------------
**Mean of the sampling exponential distribution**
---------------------------------------------------
The mean of the sampling distribution (of 40 observations from 1000 simulations) is computed step by step in order to make the computation process clearer and easy to follow.
First, build a dataset of 40 observations from 1000 simulations. The data is stored in a 1000 X 40 matrix in which each row represents a simulation of 40 observations.
```{r}
set.seed(1) ## For reproducibility
simulationDataset <- matrix(rexp(n = numberOfObservations*numberOfSimulations, rate = lamda),
nrow = numberOfSimulations, ncol = numberOfObservations)
```
Second, compute the means of each simulation of 40 observations, which will lead to 1000 means.
```{r}
simulationMeans <- rowMeans(simulationDataset)
```
Third, compute the mean of the sampling exponential distribution (mean of the 1000 means computed above).
```{r}
samplingMean <- mean(simulationMeans)
```
Sampling Mean = **`r samplingMean`**
----------------
**Discussion**
----------------
The distribution is centered at aroun 5 and the mean estimate of the 1000 sample means of the distribution (i.e. the sampling mean) is **`r samplingMean`**, which is very close to the theoritical mean of the distribution (**`r theoreticalMean`**). This is also an evidence of the Law of Large Numbers (LLN), which states that the sample mean of iid sample is consistent for the population mean.
2. Sample variance vs theoretical variance of the distribution
------------------------------------------------------------------
```{r}
theoreticalSD <- 1/lamda ## Standard Deviation
theoreticalVariance <- theoreticalSD^2 ## Variance
```
The theoretical variance of the distribution is **`r theoreticalVariance`**
```{r}
samplingVariance <- var(simulationMeans)
sigmaSquare <- numberOfObservations * samplingVariance
```
The Variance of the sample of means of  exponential distribution (variance of the 1000 means) is **`r samplingVariance`**
The variance of the sample of means is **`r samplingVariance`**, which is quite small meaning that the sample means are very centered around the sampling mean. Also this **`r samplingVariance`** sampling variance is very small compared to the theoretical variance **`r theoreticalVariance`**. However, according to CLT... $Var(sample of Means) = \frac{\sigma^2}{numberOfObservations}$ = **`r samplingVariance`**
... which means...  $\sigma^2 = numberOfObservations * Var(sample of Means)$
= **`r numberOfObservations` * `r samplingVariance`**
= **`r sigmaSquare`**
So, the estimated variance, which is **`r sigmaSquare`** is a good estimate of the theoretical variance **`r theoreticalVariance`**.
3. The distribution is approximately normal
-------------------------------------------
In order to check whether the distribution is approximately normal it is useful to compare the distribution of random exponentials to the distribution of random means of exponential distribution.
```{r}
## Split the device into a 1x1 matric to plot the 2 distributions
## Fit density associated density curve to each histogram
## Fit the mean lines to associated to each histogram
par(mfrow = c(1, 2))
hist(simulationDataset, freq = F, col = "grey", main = "Exponential Distribution")
lines(density(simulationDataset), lwd = 2)
abline(v = theoreticalMean, col = "orange", lwd = 3)
legend("topright", legend = c("Exponential Distribution Density", paste("Mean=", theoreticalMean)), cex = 0.75, col = c("black", "orange"), lwd = 3)
hist(simulationMeans, freq = F, col = "green", main = "Distribution of Means")
lines(density(simulationMeans), lwd = 2)
abline(v = samplingMean, col = "red", lwd = 3)
legend("topright", legend = paste("Mean=", round(samplingMean, 5)),cex = 0.75 , col = "red", lwd = 3)
```
From the histograms it can be clearly seen that the sample means distribution of the exponential distribution is approximately Gaussian and it looks much more normal than the exponential distribution.
plot(simulationMeans, samplingVariance, t="l")
plotCI(x=sampleMeans, uiw=samplingVariance, col="black", barcol="black", lwd=1, pch=18,
cex=2, xaxt="n", yaxt="n", xlim=c(.5,3.5), ylim=c(0, 60), ylab='AEA(pmlo/mL)',
xlab='FAAH Rare Varient Group', yaxs = 'i'
plotCI(x=sampleMeans, uiw=samplingVariance, col="black", barcol="black", lwd=1, pch=18,
cex=2, xaxt="n", yaxt="n", xlim=c(.5,3.5), ylim=c(0, 60), ylab='AEA(pmlo/mL)',
xlab='FAAH Rare Varient Group', yaxs = 'i')
?plotCI
curve(dnorm(x = simulationMeans, mean = samplingMean, sd = sqrt(samplingVariance)), from = 3, to = 8, col='red', add=T)
xfit <- seq(2, 8, 0.1)
yfit <- dnorm(xfit, mean = theoreticalMean, sd = theoreticalSD)
hist(simulationMeans, breaks = 40, prob = TRUE,
xlab = "",
main = "Sample Density compared to Normal Distribution")
lines(density(simulationMeans), col = "RED", lwd = 4)
lines(xfit, yfit, col="blue", lwd = 4)
legend("topright", c("Normal Distribution", "Sample Density"), fill = c("blue","red"))
xfit <- seq(2, 8, 0.1)
yfit <- dnorm(xfit)
hist(simulationMeans, breaks = 40, prob = TRUE,
xlab = "",
main = "Sample Density compared to Normal Distribution")
lines(density(simulationMeans), col = "RED", lwd = 4)
lines(xfit, yfit, col="blue", lwd = 4)
legend("topright", c("Normal Distribution", "Sample Density"), fill = c("blue","red"))
xfit <- seq(2, 8, 0.1)
yfit <- dnorm(xfit)
hist(simulationMeans, breaks = 40, prob = TRUE,
xlab = "",
main = "Sample Density compared to Normal Distribution")
lines(density(simulationMeans), col = "RED", lwd = 4)
lines(xfit, yfit, col="blue", lwd = 4)
legend("topright", c("Normal Distribution", "Sample Density"), fill = c("blue","red"))
xfit <- seq(min(simulationMeans), max(simulationMeans), length=100)
yfit <- dnorm(xfit, mean=theoreticalMean, sd=theoreticalSD)
lines(xfit, yfit, pch=22, col="red", lty=1)
xfit <- seq(min(simulationMeans), max(simulationMeans), length=100)
yfit <- dnorm(xfit, mean=theoreticalMean, sd=theoreticalSD)
lines(xfit, yfit, pch=22, col="red", lty=1)
library(manipulate)
myPlot <- function(s)
{
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s)
{
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot, s = slider(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s)
{
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s)
{
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
library(manipulate)
myPlot <- function(s)
{
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
install.packages("Rtools")
install.packages("shimy")
install.packages("shiny")
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text')
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text'),
h3('Sidebar'),
),
mainPanel(
h3('Main Panel text')
)
))
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
find_rtools()
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='hachimi', token='E55C6F193CC6EAF599E42CD8176219AB', secret='AiRvGao9pf07184vQmALAibNHaJL3HUzgX4N4KqZ')
install.packages("GGally")
library(GGally)
ggpairs(data = mtcars, title = "Motor Trend Cars Data", color = am)
# Loanding the data ...
data(mtcars)
# Make these variables factors
mtcars$cyl <- factor(mtcars$cyl)
mtcars$vs <- factor(mtcars$vs)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
mtcars$am <- factor(mtcars$am,labels=c('Automatic','Manual'))
library(GGally)
ggpairs(data = mtcars, title = "Motor Trend Cars Data", color = am)
ggpairs(data = mtcars, title = "Motor Trend Cars Data", color = "am")
?pch
?pairs()
?mtcars
mean(mtcars$am=="manual")
mean(mtcars$mpg[, am=="Manual"])
mean(mtcars$mpg[, mtcars$am=="Manual"])
mean(mtcars$mpg[, mtcars$am==0])
mean(mtcars$mpg[mtcars$am=="Manual"])
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(concrete)
hist(concrete$Superplasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(training)
grep("^ IL", names(training), value = TRUE)
grep("^IL", names(training), value = TRUE)
sum(grep("^IL", names(training), value = TRUE))
sum(grep("^IL", training, value = TRUE))
sum(grep("^IL", training, value = FALSE))
length(grep("^IL", names(training), value = TRUE))
?preProcess()
preProcess(training[, grep("^IL", names(training))], thresh = 0.90, method = "PCA")
temps <- training[, grep("^IL", names(training))]
View(temps)
preProcess(temps, thresh = 0.90, method = "PCA")
preProcess(temps, thresh = 0.90, method = "pca")
temps <- training[, grep("^diagnosis|IL", names(training))]
View(temps)
preProc <- preProcess(temps, thresh = 0.80, method = "pca")
preProc <- preProcess(temps[, -temps$diagnosis], thresh = 0.80, method = "pca")
preProc <- preProcess(temps[, -1], thresh = 0.80, method = "pca")
preProc
train(preProc, training[, -1])
train(preProc, training[, -1])
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
View(segmentationOriginal)
trainSet <- segmentationOriginal[, "Case" = "Train"]
trainSet <- segmentationOriginal[, segmentationOriginal$Case = "Train"]
trainSet <- segmentationOriginal[, segmentationOriginal$Case == "Train"]
trainSet <- subset(segmentationOriginal, segmentationOriginal == "Train")
testSet <- subset(segmentationOriginal, segmentationOriginal$Case == "Test")
trainSet <- subset(segmentationOriginal, segmentationOriginal$Case == "Train")
dim(segmentationOriginal)
dim(trainSet)
dim(testSet)
set.seed(125)
modelFit <- train(Class ~ ., data = trainSet, method == "rpart")
modelFit <- train(Class ~ ., data = trainSet, method = "rpart")
modelFit <- train(Class ~ ., data = trainSet, method = "rpart")
modelFit
rpart(Class ~ ., data = trainSet)
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmn")
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
dim(olive)
View(olive)
classTreeModel <- rpart(Area ~ ., data = olive, method = "class")
classTreeModel
newdata = as.data.frame(t(colMeans(olive)))
View(newdata)
plot(classTreeModel)
text(classTreeModel, cex=.75)
plot(modelFit)
plot(rpart(Class ~ ., data = trainSet))
text(modelFit, cex=.75)
text(rpart(Class ~ ., data = trainSet), cex=.75)
predict(classTreeModel, newdata = newdata, type = "class")
mynewdata <- as.data.frame(t(colMeans(olive)))
predict(classTreeModel, newdata = mynewdata)
predict(classTreeModel, newdata = mynewdata, type = "class")
predict(classTreeModel, newdata = mynewdata, type = "tree")
treeFit <- tree(Area ~ ., data = olive, method = "class")
treeFit <- train(Area ~ ., data = olive, method = "rpart")
predict(treeFit, newdata = mynewdata, type = "class")
predict(treeFit, newdata = mynewdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
View(train)
View(trainSA)
set.seed(13234)
trainFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = "glm", family = "binomial")
trainFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, method = "glm", family = "binomial")
logitFit <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, family = "binomial")
logitFit <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, family = "binomial")
logitFitTest <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = testSA, family = "binomial")
prediction <- predict(logitFit, trainSA, type = "response")
missClass = function(trainSA$chd, prediction){sum(((prediction > 0.5)*1) != trainSA$chd)/length(trainSA$chd)}
missClass = function(trainSA, prediction){sum(((prediction > 0.5)*1) != trainSA)/length(trainSA)}
missClass
missClass(trainSA = trainSA$chd, prediction)
missClass(trainSA = testSA$chd, prediction)
predictionTest <- predict(logitFitTest, testSA, type = "response")
missClass(trainSA = testSA$chd, prediction)
missClass(trainSA = testSA$chd, predictionTest)
missClass = function(testSA, predictionTest){sum(((predictionTest > 0.5)*1) != testSA)/length(testSA)}
missClass(testSA = testSA$chd, predictionTest)
predictionTest <- predict(logitFit, testSA, type = "response")
missClass(testSA = testSA$chd, predictionTest)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
View(vowel.train)
vowel.train <- as.factor(vowel.train$y)
View(vowel.train)
vowel.train$y <- as.factor(vowel.train$y)
vowel.train[, "y"] <- as.factor(vowel.train$y)
data(vowel.train)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
View(vowel.train)
set.seed(33833)
frModelFit <- train(y ~ ., data = vowel.train, method = "rf", prox = TRUE)
frModelFit
?varImp()
varImp(frModelFit)
trainingData <- read.csv("pml-training.csv", header = TRUE)
testingData <- read.csv("pml-testing.csv", header = TRUE)
setwd("~/My-DataScienceSpecialisation-Coursera/Practical Machine Learning")
View(trainin)
View(trainin)
trainingData <- read.csv("pml-training.csv", header = TRUE)
testingData <- read.csv("pml-testing.csv", header = TRUE)
View(trainingData)
trainingData <- read.csv("pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
testingData <- read.csv("pml-testing.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"))
View(trainingData)
